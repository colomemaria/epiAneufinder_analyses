import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import fargv

#Script for calculating the agreement between the full SNU601 dataset and the different sub-samplings of the dataset.
#Input files are the tables generated by epiAneufinder

unfold_pd = lambda x: (x.columns.to_numpy()[3:], np.array([f"{row[0]},{row[1]},{row[2]}" for row in x.to_numpy()[:,:3]]), x.to_numpy()[:, 3:].astype(
        np.int16))
np_union_id = lambda x,y:np.sort(np.unique(np.concatenate([x,y],axis=0)))

#def np_index_bystr(str_index, )

def index_of(large, small):
    large=large.tolist()
    return np.array([large.index(s) for s in small.tolist()], dtype=np.int32)

#Function for reading the tables and creating numpy arrays
def load_cnv(csv_path):
    full = pd.read_csv(csv_path, sep=" ")
    #print(full)
    a=full.loc[:, ~full.columns.isin(['seq', 'start', 'end'])] + 1
    b=full.loc[:, full.columns.isin(['seq', 'start', 'end'])]
    new_full=b.join(a)
    cell_ids, location_ids, values = unfold_pd(new_full)
    #print(new_full)
    return new_full, cell_ids, location_ids, values

#Function for identifing the common bins and calculating the union set consisting of the cell_id, location and their values.
#The values are encoded in order to show whether there was a change or not in the bin.
#The encoding is reference_value+(4*sampled_value)
def get_distance_values(ref, sample, offset=4):
    ref_cell_ids, ref_location_ids, ref_values = unfold_pd(ref)
    smpl_cell_ids, smpl_location_ids, smpl_values = unfold_pd(sample)
    union_cell_ids = np_union_id(ref_cell_ids, smpl_cell_ids)
    union_location_ids = np_union_id(ref_location_ids, smpl_location_ids)
    union_match = np.zeros([len(union_location_ids), len(union_cell_ids)])
    #union_match[index_of(union_cell_ids,ref_cell_ids),:][:,index_of(union_location_ids,ref_location_ids)] = ref_values
    ref_cell_ids=index_of(union_cell_ids, ref_cell_ids)
    ref_location_ids=index_of(union_location_ids, ref_location_ids)
    #union_match[ref_cell_ids,:][:,ref_location_ids] = ref_values
    union_match[np.ix_(ref_location_ids, ref_cell_ids)] = ref_values


    smpl_cell_ids=index_of(union_cell_ids, smpl_cell_ids)
    smpl_location_ids=index_of(union_location_ids, smpl_location_ids)
    union_match[np.ix_(smpl_location_ids, smpl_cell_ids)] = union_match[np.ix_(smpl_location_ids, smpl_cell_ids)] + offset * smpl_values
    #union_match[index_of(union_cell_ids, smpl_cell_ids), :][:,index_of(union_location_ids, smpl_location_ids)] += offset * smpl_values
    return union_cell_ids, union_location_ids, union_match

#Function for calculating the bins that either maintain their status between the two datsets or change, as well as the direction of the change.
#In addition it calculates the precition and recall values for the Gain/Loss/Disomic.
def render_results(union_values):
    consistent_cnv = (union_values==5) | (union_values==15)
    inconsistent_cnv = (union_values == 9) | (union_values == 13) | (union_values == 6) | (union_values == 14) | (union_values == 7)| (union_values == 11)
    print(f"1 to 2:{(union_values==9).sum()}")
    print(f"1 to 3:{(union_values == 13).sum()}")
    print(f"2 to 1:{(union_values == 6).sum()}")
    print(f"2 to 3:{(union_values == 14).sum()}")
    print(f"3 to 1:{(union_values == 7).sum()}")
    print(f"3 to 2:{(union_values == 11).sum()}")
    print(f"Consistent {consistent_cnv.sum()}\tInconsistent{inconsistent_cnv.sum()}\t{100*consistent_cnv.sum()/(consistent_cnv.sum()+inconsistent_cnv.sum())}")
    precissionLoss=(union_values==5).sum()/((union_values==5).sum()+(union_values==7).sum()+(union_values==6).sum())
    precisionGain=(union_values == 15).sum() / ((union_values == 15).sum() + (union_values == 13).sum() + (union_values == 14).sum())
    presicionBase=(union_values == 10).sum() / ((union_values == 10).sum() + (union_values == 9).sum() + (union_values == 11).sum())
    print(f"PrecisionLoss:{precissionLoss}")
    print(f"PrecisionGain:{precisionGain}")
    print(f"PrecisionBase:{presicionBase}")
    recallGain=(union_values == 15).sum() / ((union_values == 15).sum() + (union_values == 7).sum() + (union_values == 11).sum())
    recallBase=(union_values == 10).sum() / ((union_values == 10).sum() + (union_values == 14).sum() + (union_values == 6).sum())
    recallLoss=(union_values == 5).sum() / ((union_values == 5).sum() + (union_values == 9).sum() + (union_values == 13).sum())
    print(f"RecallGain:{recallGain}")
    print(f"RecallLoss:{recallLoss}")
    print(f"RecallBase:{recallBase}")

#Calculation of the results per cell
def render_results_per_cell(union_values):
    table=pd.DataFrame(union_values)
    count_table=table.apply(pd.Series.value_counts)
    count_table=count_table.fillna(0)
    return count_table

if __name__ =="__main__":
    p = {"ref_cnv":"/tmp/1.csv", "sample_cnv":"/tmp/2.csv"}
    p, _ = fargv.fargv(p)
    ref_full, ref_cell_ids, ref_location_ids, ref_values = load_cnv(p.ref_cnv)
    smpl_full, smpl_cell_ids, smpl_location_ids, smpl_values = load_cnv(p.sample_cnv)
    union_cell_ids, union_location_ids, union_match = get_distance_values(ref_full, smpl_full)
    render_results(union_values=union_match)
    count_table=render_results_per_cell(union_values=union_match)
    print(count_table)
    #Creation of 3 files (for gain, loss, disomic) with the percentage of lost status for each cell
    lost_losses=100*((count_table.loc[9] + count_table.loc[13]) /(count_table.loc[9] + count_table.loc[13]+ count_table.loc[5])).fillna(-100)
    ##lost_losses = 100 * ((count_table.loc[9]) / (count_table.loc[9]  + count_table.loc[5])).fillna(-100)
    lost_losses.to_csv(p.sample_cnv.split(".csv")[0]+"losses_lost.txt",index=False,header=False)

    gains_lost=100*((count_table.loc[7] + count_table.loc[11]) /(count_table.loc[7] + count_table.loc[11]+ count_table.loc[15])).fillna(-100)
    ##gains_lost = 100 * ((count_table.loc[11]) / ( count_table.loc[11] + count_table.loc[15])).fillna(-100)
    gains_lost.to_csv(p.sample_cnv.split(".csv")[0]+"gains_lost.txt",index=False,header=False)

    disomy_lost=100*((count_table.loc[6] + count_table.loc[14]) /(count_table.loc[6] + count_table.loc[14]+ count_table.loc[10])).fillna(-100)
    disomy_lost.to_csv(p.sample_cnv.split(".csv")[0]+"disomy_lost.txt",index=False,header=False)

    similar=100*((count_table.loc[5]+count_table.loc[15]+count_table.loc[10])/(count_table.loc[5]+count_table.loc[15]+count_table.loc[10]+count_table.loc[9] + count_table.loc[13]+count_table.loc[7] + count_table.loc[11]+count_table.loc[6] + count_table.loc[14])).fillna(-100)
    similar.to_csv(p.sample_cnv.split(".csv")[0]+"similarity.txt",index=False,header=False)


